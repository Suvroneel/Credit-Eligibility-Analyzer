# Suvroneel Nathak â€” The Loan Eligibility Engine

This repository contains my submission for the **SDE Intern (Backend)** assignment.  
It showcases a backend pipeline that ingests user CSVs, discovers relevant loan products, 
matches users to eligible loans based on rules, and triggers notifications.

The solution is implemented using n8n automations, Docker, Python, and a structured SQL schema.  
AWS services are prepared for deployment but are optional for local testing.

---

## ğŸ“ Repository Structure

- `lambdas/` â€“ AWS Lambda functions (presigned URL generator, S3 CSV processor)
- `infra/` â€“ Serverless Framework configuration and infra notes
- `n8n/` â€“ n8n workflow JSONs + docker-compose setup
  - `n8n/workflows/user_loan_workflow.json` â€“ exported n8n workflow
- `ui/` â€“ Minimal UI for uploading CSV files
- `sql/` â€“ Database schema, seed queries, helper queries
- `docs/` â€“ Architecture diagram, workflow screenshot, design decisions, and deployment notes

---

## ğŸ—ï¸ Architecture Diagram

![Architecture Diagram](docs/Architecture.jpg)

---

## ğŸ“Š n8n Workflow

The automation workflow is built in n8n.  
- **Screenshot:**  
  ![n8n Workflow](docs/n8n_workflow.png)  
- **Workflow JSON:**  
  `n8n/workflows/user_loan_workflow.json`

The workflow includes the following nodes in order:  
1. **Schedule Trigger** â€“ triggers workflow at defined intervals  
2. **Read from Disk** â€“ reads the uploaded CSV file  
3. **Extract from File** â€“ converts CSV data into JSON format  
4. **Code (Python) Node** â€“ processes user data for loan eligibility logic  
5. **PostgreSQL (Insert Rows) Node** â€“ inserts processed data into database tables

---

## ğŸš€ Quick Start (Local Development)

1. **Start PostgreSQL locally:**
   ```bash
   docker run --name loan-postgres \
     -e POSTGRES_PASSWORD=postgres \
     -e POSTGRES_DB=loan_db \
     -p 5432:5432 \
     -d postgres:15
